{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corpus_raw = 'He is the king . The king is royal . she is the royal queen .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the king . the king is royal . she is the royal . queen he is the king . the king is royal . she is the royal queen\n"
     ]
    }
   ],
   "source": [
    "# Convert to lower case \n",
    "corpus_raw = corpus_raw.lower()\n",
    "print corpus_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here’s a handy example from this amazing post on word2vec by Chris McCormick.\n",
    "#from IPython.display import Image\n",
    "#Image(filename='/home/asabir/Desktop/exp-w2v.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "#becuase we dont want to treat . as word \n",
    "for word in corpus_raw.split():\n",
    "    if word != '.':\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['king', 'is', 'queen', 'royal', 'she', 'the', 'he'])\n"
     ]
    }
   ],
   "source": [
    "# remove all duplicate word \n",
    "words = set (words)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give the total number of unique words\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "# Give the total number of unique words \n",
    "vocab_size = len(words)\n",
    "print('give the total number of unique words')\n",
    "print vocab_size \n",
    "### \n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i \n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# The dictionaries allow us to do : \n",
    "print (word2int['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "royal\n"
     ]
    }
   ],
   "source": [
    "print (int2word[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal'], ['queen', 'he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "# we want a list if our sentences as a list of words:\n",
    "# raw_sentencce is a list of sentences\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = [] \n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "# to give us a list of sentences where each senteance is a list of words. \n",
    "print (sentences)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we genterate our traning data \n",
    "data = [] \n",
    "\n",
    "WINDOW_SIZE = 2 \n",
    "\n",
    "for sentence in sentences: \n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[(max(word_index - WINDOW_SIZE, 0)): min(word_index + WINDOW_SIZE, len(sentence)+1)] :\n",
    "                             if nb_word != word: \n",
    "                                   data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is'], ['is', 'he'], ['is', 'the'], ['the', 'he'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['the', 'king'], ['king', 'the'], ['king', 'is'], ['is', 'the'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['she', 'is'], ['is', 'she'], ['is', 'the'], ['the', 'she'], ['the', 'is'], ['the', 'royal'], ['royal', 'is'], ['royal', 'the'], ['queen', 'he'], ['he', 'queen'], ['he', 'is'], ['is', 'queen'], ['is', 'he'], ['is', 'the'], ['the', 'he'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['the', 'king'], ['king', 'the'], ['king', 'is'], ['is', 'the'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['she', 'is'], ['is', 'she'], ['is', 'the'], ['the', 'she'], ['the', 'is'], ['the', 'royal'], ['royal', 'is'], ['royal', 'the'], ['royal', 'queen'], ['queen', 'the'], ['queen', 'royal']]\n"
     ]
    }
   ],
   "source": [
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we need to represented the data in ONE-hot-encoder \n",
    "# for example \n",
    "# for example you have 3 words : tokyo, texas , barcelona\n",
    "#word2int = ['tokyo'] --> 0 --> [1 0 0]\n",
    "#word2int = ['barcelona'] --> 1 --> [0 1 0]\n",
    "#word2int = ['texas '] --> 2 --> [0 0 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hot encoder vector \n",
    "# function to convert number to one hot vertors \n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index]= 1 \n",
    "    return temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input word \n",
    "x_train = [] \n",
    "# output word\n",
    "y_train = []\n",
    "# \n",
    "for data_word in data: \n",
    "    x_train.append(to_one_hot(word2int[data_word[0]], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[data_word[1]], vocab_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert them to numpy arrays \n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((54, 7), (54, 7))\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "### meaning 27 tranining point where each point has 7 dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Tensorflow program start Here \n",
    "### creating the placeholder  x_train, and y_train \n",
    "x = tf.placeholder(tf.float32, shape =(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape =(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='/home/asabir/Desktop/Embaddig.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_29:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_28:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='/home/asabir/Desktop/Embaddig2.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we convert it into embadding representation (as figure　↓し)\n",
    "## you can choose the your own number \n",
    "EMBEDDING_DIM = 5 \n",
    "### W\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "### B \n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "## Hidden \n",
    "hidden_representation = tf.add(tf.matmul(x, W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='/home/asabir/Desktop/Embaddig3.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Input_one_hot ---> embedded repr. ----> predicted_neighbour_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let start the tranining  \n",
    "sess = tf.Session() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the loss function \n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the tranining stetp \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss is : ', 3.7491734)\n",
      "('loss is : ', 3.5242376)\n",
      "('loss is : ', 3.3510962)\n",
      "('loss is : ', 3.2079258)\n",
      "('loss is : ', 3.0847187)\n",
      "('loss is : ', 2.9762363)\n",
      "('loss is : ', 2.8792386)\n",
      "('loss is : ', 2.7914884)\n",
      "('loss is : ', 2.7113633)\n",
      "('loss is : ', 2.6376729)\n",
      "('loss is : ', 2.5695522)\n",
      "('loss is : ', 2.5063887)\n",
      "('loss is : ', 2.4477642)\n",
      "('loss is : ', 2.3934081)\n",
      "('loss is : ', 2.3431528)\n",
      "('loss is : ', 2.2968962)\n",
      "('loss is : ', 2.2545612)\n",
      "('loss is : ', 2.2160583)\n",
      "('loss is : ', 2.1812558)\n",
      "('loss is : ', 2.1499569)\n"
     ]
    }
   ],
   "source": [
    "### train for n_iter iterations \n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vectors = sess.run(W1+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63531131 -0.29805526  0.16432349  0.71807015 -0.25180089]\n",
      " [ 2.25697994 -0.58208686  1.39743459 -0.18036509 -0.03716298]\n",
      " [ 0.85124224  1.08339036 -0.99309367  0.55763459  1.092062  ]\n",
      " [-2.56640577  0.55660689 -1.1398946  -2.48028779 -0.7292859 ]\n",
      " [ 1.94922936  1.48876047 -2.22827291 -0.27098572  0.98659408]\n",
      " [-0.94583172 -1.78401351 -0.67550153 -1.35020363 -0.00434981]\n",
      " [-0.58859318 -1.23841906 -1.00922036  0.2624585   0.20980985]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print (sess.run(W1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.67084026 -0.41547573  1.34568477  0.66651189 -0.88418996]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_61:0' shape=(5,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### one-hot-vectors \n",
    "##from IPython.display import Image\n",
    "##Image(filename='/home/asabir/Desktop/hot_v.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.30615163 -0.71353102  1.51000822  1.38458204 -1.13599086]\n",
      " [ 3.92782021 -0.99756259  2.74311924  0.48614681 -0.92135292]\n",
      " [ 2.52208257  0.66791463  0.3525911   1.22414649  0.20787203]\n",
      " [-0.89556551  0.14113116  0.20579016 -1.8137759  -1.6134758 ]\n",
      " [ 3.6200695   1.07328475 -0.88258815  0.39552617  0.10240412]\n",
      " [ 0.72500855 -2.19948912  0.67018324 -0.68369174 -0.88853979]\n",
      " [ 1.08224702 -1.65389478  0.33646441  0.9289704  -0.67438012]]\n"
     ]
    }
   ],
   "source": [
    "#Multiply the one hot vector with W1, we basically get access to the raw of W1 which is in the fact ..\n",
    "# the embedded representation of the word represented by the input ONE-HOT-VECTOR. So W1 is essentially acting\n",
    "# as a look up-table \n",
    "vectors = sess.run(W1+b1)\n",
    "# if you work it out, you will see that it has the same effect as running the node hidden representation\n",
    "print vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.52208257  0.66791463  0.3525911   1.22414649  0.20787203]\n"
     ]
    }
   ],
   "source": [
    "# So, if we want the representation for 'queen; for example \n",
    "# say here word2int['queen'] is 2 \n",
    "print(vectors[ word2int['queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now we have our Vectors \n",
    "## To find the closest vector to a given vector \n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest(word_index, vectors):\n",
    "    # act loke positive infinity \n",
    "    min_dist = 1000 \n",
    "    min_index = -1 \n",
    "    \n",
    "    query_vector = vectors[word_index]\n",
    "    \n",
    "    for index, vector in enumerate (vectors):\n",
    "        if euclidean_dist(vector, query_vector)< min_dist and not np.array_equal(vector, query_vector):\n",
    "            \n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "            \n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "she\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "### NOW lets check out query of two vectors 'king' and 'royal\"\n",
    "print (int2word[find_closest(word2int['king'], vectors)])\n",
    "print (int2word[find_closest(word2int['queen'], vectors)])\n",
    "print(int2word[find_closest(word2int['royal'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our embadding learnt that \n",
    "# King is closest to he \n",
    "# queen is closest to royal \n",
    "# royal is closest to she \n",
    "#### BIGGER Corpus will lead to better results### \n",
    "### As well note that the random initialization of the weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### lets reduce the the demension from 5 to 2 with our dimensionality reduction technique: tSNE\n",
    "import sklearn \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components =2, random_state= 0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vector =model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  we need to normalize the result, so that we can view them plot them\n",
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('king', -0.21212071)\n",
      "('is', -0.19938162)\n",
      "('queen', 0.22945653)\n",
      "('royal', 0.054291688)\n",
      "('she', 0.275267)\n",
      "('the', -0.82723385)\n",
      "('he', -0.71587056)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAESCAYAAAArJ3joAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFWdsJEnFGRtJUnH/D9oHF6yHIK8ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12219ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
